---
layout: page
title: "Compress"
assignment: "huffman"
---

##Intro
The following sections will provide you with detailed step-by-step directions on how to properly implement the compression algorithm.  It is suggested that you write a short test file that you can compress by hand to compare the output of your compress method to as you go, although you may opt to develop from the junit tests instead.  Be sure to use the same strategy to build your Huffman tree as this may be an unnecessary source of confusion.  Additionally, it is highly recommended that you write a number of helper methods rather than one long method to implement compress.  It is better design and will be much easier to debug should you encounter a problem.  Furthermore, use descriptive method and variable names.  This is also good design and will be considered as part of your engineering score.

##Building the Tree
Since Huffman coding relies on character entropy/frequencies to generate new codes, the first step of your compression algorithm should be to count the number of occurrences of each character in the file.  You can read bits from the provided `BitInputStream` using `readBits(int howManyBits)`.  You should read `BITS_PER_WORD` (which equals 8) bits at a time in order to get a single character at each call.  You should continue to call `in.readBits(BITS_PER_WORD)` until it returns -1, which indicates that you've reached the end of the file.  Storing the frequencies is best done using either an `array` or a `Map`.  Since Java can automatically convert `char` and `int` primitive values, you can use the indexes of an `array` as a faster alternative to `Character` keys in a `Map` of `Character` to `Integer`, although both data structures are appropriate.  Either way, the values should hold the accumulated frequency of each character.  Be sure to reset your `BitInputStream` once you have finished reading all of the characters so that you can reread the file without error when you encode it later.  You can do this by calling `in.reset()`.

Now that you know how often each character occurs, you can construct a Huffman tree based on the file.  To construct the tree, first create a `HuffNode` for each character that occurs at least once.  You don't want to include characters that don't occur at all since this will make your tree much larger than it needs to be and minimize the effectiveness of your compression algorithm.  Remember also that the whole point of Huffman coding is to leave out the characters that don't occur at all.  The weight of each `HuffNode` should be the character's frequency and the value should be the character itself, converted automatically by Java to an `int`.  Remember to include the pseudo-eof character in the tree; its `HuffNode` should have a weight of 0 and a value of `PSEUDO_EOF`, a constant in `Processor`.  You'll need to put each `HuffNode` into a data structure of your choosing.  Since `HuffNode` implements `Comparable`, you can use either an `array` with `Arrays.sort(HuffNode[] nodes)`, some type of `List` with `Collections.sort(List nodes)`, or any other built in Java library you please.

However, the classic solution to Huffman is to use a `PriorityQueue` which will automatically sort the nodes for you and return the smallest weighted node each time you call `poll()`.  This implementation is much faster than using an `array` or a `List` since you don't have to do a full sort at each step; the only other data structure with comparable speed would be a `TreeSet` (which doesn't work since there may be `HuffNode`s with equal weights).  No matter which data structure you use, you'll need to use a while loop to continue combining nodes until you only have one left.  To combine two nodes, first remove the two smallest nodes from your data structure.  Then create a new node with the two nodes as its sub-children and their combined weight as its weight.  Put the new node back into the data structure and repeat.  The node that remains at the end is the root of your Huffman tree.
				
##Extracting the Codes
Since you have a Huffman tree in the form of a single `HuffNode`, you also have all of the Huffman codes you will need to compress the body of the file.  Although you could search the tree each time you encounter a character in the file, this method will take quite a while to compress (an O(n) operation done m times).  Even memoizing the searching only improves the run time so much (an O(n) operation done n times) where n is the number of `HuffNode`s in the tree and m is the total number of characters in the file.  Rather, it will be much more efficient to pre-process your tree and extract all of the codes before compressing the body of the file (an O(n) operation done once).  As with the frequencies, you can either use an `array` or a `Map` to store the new codes.  Populating your `array`/`Map` can be done with a basic recursive traversal.  At the base case where you reach a leaf, simply add a new entry to the `array` or `Map`.  Otherwise make a recursive call down the left and right sub-tree with the correct updated arguments.  Although in theory it does not matter whether 0 indicates the left or right, our implementation uses 0 to indicate the left and 1 to indicate the right, so in order to match our process exactly and optimize your score for Huffman, please use the same pattern.  It is advised that you store each Huffman code as a `String` of 0s and 1s to preserve the integrity of the code.  Using an `int` may preserve the value, but it cannot reliably represent the length of the code when there are leading 0s (e.g. the codes 01 and 001 would be indistinguishable in `int` form).  You may elect to add a second `array`/`Map` to separately store the number of bits in the code, although the `String` approach may be more intuitive and easier to implement with recursion.

##Writing the Header
First, you need to write the `HUFF_NUMBER`.  Writing bits is done by calling `writeBits(int howManyBits, int value)` in `BitOutputStream`.  Since `HUFF_NUMBER` is an `int`, we want to write `BITS_PER_INT` bits with the value `HUFF_NUMBER`.  Next, you need to write out the information of the tree.  This is best done recursively.  For the base case when you reach a leaf node, first write a single bit with the value 1.  This distinguishes leaf nodes from internal nodes.  Then, write out 9 bits with the value of the leaf node.  Even though it should only take 8 bits in theory to store the character value of the node, with the addition of the `PSEUDO_EOF` node it actually takes 9 bits to fully distinguish all of the possibilities.  Alternatively, for an internal node, write out a 0 and then make two recursive calls to write the left and right sub-trees in that order.  Again, while technically it doesn't matter whether you write the right or left sub-tree first (so long as you read them in the same order in decompress) use this order to match our implementation and get full credit.  As an example, the header for the "go go gophers" tree above would look something like:

`0 0 0 1 --S-- 1 --_-- 0 0 1 --P-- 1 --H-- 0 1 --E-- 1 --R-- 0 1 --G-- 1 --O--`
The space are added for readability, and the --x--s correspond to the 9-bit version of the ASCII codes for each character.

##Writing the Body
You're nearly done with compression; all that's left is writing the encoded body of the file.  Similar to when you first read the file while counting character frequencies, you need to read `BITS_PER_WORD` bits at a time to isolate each character.  For each character, use your `Map` or `array` to obtain the Huffman code.  You will call `writeBits()` again, using the length of the `String` as the first parameter (or the value from the secondary `Map`/`array` holding the length if you made that choice) and the `int` value of the `String` as the second parameter.  You can use `Integer.parseInt(String value, int radix)` to do this.  The `String` value you obtain from the `Map`/`array` and the radix should be 2 since the number represented by the `String` is in binary.  Once you have written out every character from the original file, make one last `writeBits()` call using the `PSEUDO_EOF` entry in your `Map` or `array` to complete the compression process.
